{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "BiLSTM Non Disruption.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Dw-VT84wpO"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Annotated_data_nondisrupt_test_train.csv')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Xivdr9e755RY",
        "outputId": "2b8f008e-2223-415f-e8be-3debdd02b142"
      },
      "source": [
        "df = df.sample(frac = 1)\n",
        "df = df.reset_index(drop = True)\n",
        "df"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>incident</th>\n",
              "      <th>Labels</th>\n",
              "      <th>TrainOrTest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7614</td>\n",
              "      <td>7651.0</td>\n",
              "      <td>A Franconia-Springfield-bound Blue Line train...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>62631</td>\n",
              "      <td>62725.0</td>\n",
              "      <td>A Vienna-bound Orange Line train at New Carrol...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21265</td>\n",
              "      <td>21322.0</td>\n",
              "      <td>A New Carrollton-bound Orange Line train at V...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7628</td>\n",
              "      <td>7665.0</td>\n",
              "      <td>A Largo Town Center-bound Silver Line train a...</td>\n",
              "      <td>Environment</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>74983</td>\n",
              "      <td>NaN</td>\n",
              "      <td>really annoyed that my @user stuff hasn't arri...</td>\n",
              "      <td>Non Disruption</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81719</th>\n",
              "      <td>35860</td>\n",
              "      <td>35929.0</td>\n",
              "      <td>A Largo Town Center-bound Blue Line train outs...</td>\n",
              "      <td>Track</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81720</th>\n",
              "      <td>18873</td>\n",
              "      <td>18930.0</td>\n",
              "      <td>A Vienna-bound Orange Line train at Cheverly ...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81721</th>\n",
              "      <td>22999</td>\n",
              "      <td>23056.0</td>\n",
              "      <td>Orange, Silver and Blue Line trains were sing...</td>\n",
              "      <td>Track</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81722</th>\n",
              "      <td>61297</td>\n",
              "      <td>61389.0</td>\n",
              "      <td>A Largo Town Center-bound Blue Line train at R...</td>\n",
              "      <td>Mechanical</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81723</th>\n",
              "      <td>40510</td>\n",
              "      <td>40585.0</td>\n",
              "      <td>A New Carrollton-bound Orange Line train at Vi...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>81724 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  Unnamed: 0.1  ...          Labels TrainOrTest\n",
              "0            7614        7651.0  ...     Operational        test\n",
              "1           62631       62725.0  ...     Operational       train\n",
              "2           21265       21322.0  ...     Operational       train\n",
              "3            7628        7665.0  ...     Environment       train\n",
              "4           74983           NaN  ...  Non Disruption       train\n",
              "...           ...           ...  ...             ...         ...\n",
              "81719       35860       35929.0  ...           Track       train\n",
              "81720       18873       18930.0  ...     Operational        test\n",
              "81721       22999       23056.0  ...           Track       train\n",
              "81722       61297       61389.0  ...      Mechanical       train\n",
              "81723       40510       40585.0  ...     Operational       train\n",
              "\n",
              "[81724 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkzyhv-7BpS3",
        "outputId": "e8c1fa57-00ad-41ce-b951-eeab8331c728"
      },
      "source": [
        "df.Labels[df.TrainOrTest == 'train'].value_counts()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Operational       22962\n",
              "Mechanical        18029\n",
              "Non Disruption    14931\n",
              "Track              5194\n",
              "Environment        3809\n",
              "Security           2287\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qbUTKIdBrMG",
        "outputId": "0b30d00b-e59e-4521-bb2a-9ffb161dfa8c"
      },
      "source": [
        "df.Labels.value_counts()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Operational       28612\n",
              "Mechanical        22212\n",
              "Non Disruption    16372\n",
              "Track              6660\n",
              "Environment        4831\n",
              "Security           3037\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glS9zx0e5535"
      },
      "source": [
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPWtwq0Y6RCK"
      },
      "source": [
        "#Test = df[df['TrainOrTest'] == 'test'].reset_index(drop = True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHag9aGr6bg0"
      },
      "source": [
        "#Train = df[df['TrainOrTest'] == 'train'].reset_index(drop = True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJIHjIi6j_n"
      },
      "source": [
        "del df['TrainOrTest']"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yk0cBCwUQiy"
      },
      "source": [
        "df = df.rename(columns={'incident': 'tweet', 'Labels': 'label'})"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dqUXMQz7is0",
        "outputId": "a0ed8023-f1b3-4bf5-a939-a4a3c76bcc4d"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LCdTuScg6212",
        "outputId": "7d17acfb-1037-42e0-9340-70cc63e72916"
      },
      "source": [
        "#importing required libraries\n",
        "import nltk\n",
        "import inflect\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import re, string, unicodedata\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First function is used to denoise text\n",
        "def denoise_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "sample_text = \"<p>he didn't say anything </br> about what's gonna <html> happen in the climax\"\n",
        "denoise_text(sample_text)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'he did not say anything  about what is going to  happen in the climax'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFesDthT7Idc",
        "outputId": "f1dbd521-f7a6-4ed1-95f6-925deb1ef9d6"
      },
      "source": [
        "# Text normalization includes many steps.\n",
        "# Each function below serves a step.\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "def normalize_text(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    #words = stem_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "    return words\n",
        "# Testing the functions\n",
        "print(\"remove_non_ascii results: \", remove_non_ascii(['h', 'ॐ', '©', '1']))\n",
        "print(\"to_lowercase results: \", to_lowercase(['HELLO', 'hiDDen', 'wanT', 'GOING']))\n",
        "print(\"remove_punctuation results: \", remove_punctuation(['hello!!', 'how?', 'done,']))\n",
        "print(\"replace_numbers results: \", replace_numbers(['1', '2', '3']))\n",
        "print(\"remove_stopwords results: \", remove_stopwords(['this', 'and', 'amazing']))\n",
        "print(\"stem_words results: \", stem_words(['beautiful', 'flying', 'waited']))\n",
        "print(\"lemmatize_verbs results: \", lemmatize_verbs(['hidden', 'walking', 'ran']))\n",
        "print(\"normalize_text results: \", normalize_text(['hidden', 'in', 'the', 'CAVES', 'he', 'WAited', '2', 'ॐ', 'hours!!']))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "remove_non_ascii results:  ['h', '', '', '1']\n",
            "to_lowercase results:  ['hello', 'hidden', 'want', 'going']\n",
            "remove_punctuation results:  ['hello', 'how', 'done']\n",
            "replace_numbers results:  ['one', 'two', 'three']\n",
            "remove_stopwords results:  ['amazing']\n",
            "stem_words results:  ['beauty', 'fly', 'wait']\n",
            "lemmatize_verbs results:  ['hide', 'walk', 'run']\n",
            "normalize_text results:  ['hide', 'cave', 'wait', 'two', 'hours']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aiQS-m78A7e",
        "outputId": "aa341e88-962d-4c5a-c5fe-1f643406b7e6"
      },
      "source": [
        "# Tokenize tweet into words\n",
        "nltk.download('punkt')\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "# check the function\n",
        "sample_text = 'he did not say anything  about what is going to  happen'\n",
        "print(\"tokenize results :\", tokenize(sample_text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "tokenize results : ['he', 'did', 'not', 'say', 'anything', 'about', 'what', 'is', 'going', 'to', 'happen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jjf-YzvF84Ta",
        "outputId": "feb17046-86f3-4668-c5bf-116615cda8e3"
      },
      "source": [
        "def text_prepare(text):\n",
        "    text = denoise_text(text)\n",
        "    text = ' '.join([x for x in normalize_text(tokenize(text))])\n",
        "    return text\n",
        "df['tweet'] = [text_prepare(x) for x in df['tweet']]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>franconiaspringfieldbound blue line train arli...</td>\n",
              "      <td>Operational</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>viennabound orange line train new carrollton o...</td>\n",
              "      <td>Operational</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>new carrolltonbound orange line train vienna o...</td>\n",
              "      <td>Operational</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>largo town centerbound silver line train lenfa...</td>\n",
              "      <td>Environment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>really annoy user stuff arrive pay extra next ...</td>\n",
              "      <td>Non Disruption</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet           label\n",
              "0  franconiaspringfieldbound blue line train arli...     Operational\n",
              "1  viennabound orange line train new carrollton o...     Operational\n",
              "2  new carrolltonbound orange line train vienna o...     Operational\n",
              "3  largo town centerbound silver line train lenfa...     Environment\n",
              "4  really annoy user stuff arrive pay extra next ...  Non Disruption"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG7XS1a6c6xx"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcgbBv_7Vnqy"
      },
      "source": [
        "#!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0WhiGmtVu_C"
      },
      "source": [
        "#!unzip glove*.zip"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEmqWKCpSK5b"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_4e-hRiSUph"
      },
      "source": [
        "df.to_csv('Preprocessed-Annotated-Disruption-NonDisruption.csv')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdkdYHT2S0qX"
      },
      "source": [
        "#df = pd.read_csv('Preprocessed-Annotated-Disruption-NonDisruption.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLv9TbQrTB1w"
      },
      "source": [
        "#df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmORGu7uFwb_"
      },
      "source": [
        "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZScwJRoiQ4Vm",
        "outputId": "3e6596f5-3feb-4c2c-b90a-62ded70f52c8"
      },
      "source": [
        "def prepare_model_input(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train_Glove = text[0:len(X_train), ]\n",
        "    X_test_Glove = text[len(X_train):, ]\n",
        "    embeddings_dict = {}\n",
        "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_dict[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (X_train_Glove, X_test_Glove, word_index, embeddings_dict)\n",
        "## Check function\n",
        "x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "x_test_sample = [\"I’m creating a macro and need some text for testing purposes\", \"I’m designing a document and don’t want to get bogged down in what the text actually says\"]\n",
        "X_train_Glove_s, X_test_Glove_s, word_index_s, embeddings_dict_s = prepare_model_input(x_train_sample, x_test_sample, 100, 20)\n",
        "print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "print(\"\\n X_test_Glove_s \\n \", X_test_Glove_s)\n",
        "print(\"\\n Word index of the word testing is : \", word_index_s[\"testing\"])\n",
        "print(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 51 unique tokens.\n",
            "(4, 20)\n",
            "Total 400000 word vectors.\n",
            "\n",
            " X_train_Glove_s \n",
            "  [[ 0  0  0  0  0  0  0  0  8  9  5 10 11  2  6  3 12  4 13 14]\n",
            " [17 18 19  1 20 21 22 23 24  3 25 26  6  1 27 28 29 30 31 32]]\n",
            "\n",
            " X_test_Glove_s \n",
            "  [[ 0  0  0  0  0  0  0  0  0  7 33  1 34  4 35 36  2 37 38 39]\n",
            " [ 0  0  0  7 40  1 41  4 42 43 44 45 46 47 48 49  3  2 50 51]]\n",
            "\n",
            " Word index of the word testing is :  38\n",
            "\n",
            " Embedding for thw word want \n",
            " \n",
            " [ 0.13627  -0.054478  0.3703   -0.41574   0.60568  -0.42729  -0.50151\n",
            "  0.35923  -0.49154   0.21827  -0.15193   0.52536  -0.24206   0.023875\n",
            "  0.8225    1.089     0.98825  -0.17803   0.77806  -1.0647   -0.28742\n",
            "  0.50458   0.21612   0.65681   0.34295  -2.1084   -0.82557  -0.31966\n",
            "  0.87567  -1.0679    3.3802    1.2084   -1.272    -0.15921  -0.25237\n",
            " -0.2696   -0.18756  -0.35523   0.084172 -0.56539  -0.24081   0.15926\n",
            "  0.3287    0.54591   0.29897   0.18948  -0.57113   0.17399  -0.19338\n",
            "  0.51921 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Z_SfPEWTJo"
      },
      "source": [
        "def build_bilstm(word_index, embeddings_dict, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5, hidden_layer = 3, lstm_node = 32):\n",
        "    model = Sequential()\n",
        "    # Make the embedding matrix using the embedding_dict\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    # Add embedding layer\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    # Add hidden layers \n",
        "    for i in range(0,hidden_layer):\n",
        "        # Add a bidirectional lstm layer\n",
        "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "        # Add a dropout layer after each lstm layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.2)))\n",
        "    model.add(Dropout(dropout))\n",
        "    # Add the fully connected layer with 256 nurons and relu activation\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    # Add the output layer with softmax activation since we have 2 classes\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    # Compile the model using sparse_categorical_crossentropy\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Neq50AaY85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c29b91c-b038-4203-97d1-883946a81cca"
      },
      "source": [
        "X = df.tweet\n",
        "y = df.label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "print(\"Preparing model input ...\")\n",
        "X_train_Glove, X_test_Glove, word_index, embeddings_dict = prepare_model_input(X_train,X_test)\n",
        "print(\"Done!\")\n",
        "print(\"Building Model!\")\n",
        "model = build_bilstm(word_index, embeddings_dict, 6)\n",
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing model input ...\n",
            "Found 27986 unique tokens.\n",
            "(81724, 500)\n",
            "Total 400000 word vectors.\n",
            "Done!\n",
            "Building Model!\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 500, 50)           1399350   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 500, 64)           21248     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 500, 64)           24832     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 500, 64)           24832     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 1542      \n",
            "=================================================================\n",
            "Total params: 1,513,276\n",
            "Trainable params: 1,513,276\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BS1ue5a0vn"
      },
      "source": [
        "def get_eval_report(labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    precision = (tp)/(tp+fp)\n",
        "    recall = (tp)/(tp+fn)\n",
        "    f1 = (2*(precision*recall))/(precision+recall)\n",
        "    return {\n",
        "        \"mcc\": mcc,\n",
        "        \"true positive\": tp,\n",
        "        \"true negative\": tn,\n",
        "        \"false positive\": fp,\n",
        "        \"false negative\": fn,\n",
        "        \"pricision\" : precision,\n",
        "        \"recall\" : recall,\n",
        "        \"F1\" : f1,\n",
        "        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n",
        "    }\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(labels, preds)\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOisJquhUebN",
        "outputId": "cfbfe5cc-048a-4a29-a8dd-8605444b5087"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='loss', verbose=1,\n",
        "    save_best_only=True, mode='auto', period=1)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efP0msqhaZdR",
        "outputId": "ebe04b42-71c0-44f5-9828-c67696a71675"
      },
      "source": [
        "history = model.fit(X_train_Glove, y_train,\n",
        "                           validation_data=(X_test_Glove,y_test),\n",
        "                           epochs=2,\n",
        "                           batch_size=128,\n",
        "                           verbose=1, callbacks=[checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "  4/511 [..............................] - ETA: 44:55 - loss: 0.7947 - accuracy: 0.7637"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otrjIaXVUFKj"
      },
      "source": [
        "model.load_weights(\"best_model.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3qgZkrz_ZFf"
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n",
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6-hk4Yq_d14"
      },
      "source": [
        "print(\"\\n Evaluating Model ... \\n\")\n",
        "predicted = model.predict_classes(X_test_Glove)\n",
        "print(metrics.classification_report(y_test, predicted))\n",
        "print(\"\\n\")\n",
        "logger = logging.getLogger(\"logger\")\n",
        "result = compute_metrics(y_test, predicted)\n",
        "for key in (result.keys()):\n",
        "    logger.info(\"  %s = %s\", key, str(result[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WURcbeP6q2Ic"
      },
      "source": [
        "#https://medium.com/analytics-vidhya/building-a-text-classification-model-using-bilstm-c0548ace26f2\n",
        "#https://github.com/pashupati98/Text-Classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jlcJW3Vva5eU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
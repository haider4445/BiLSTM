{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BiLSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-Dw-VT84wpO"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('Annotated_data_all_na_free_test_train.csv')"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "Xivdr9e755RY",
        "outputId": "841a8885-8401-4e10-b4c0-77540d6006af"
      },
      "source": [
        "df"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>incident</th>\n",
              "      <th>Labels</th>\n",
              "      <th>TrainOrTest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>A Fort Totten-bound Yellow Line train at Brad...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>A Grosvenor-bound Red Line train at Gallery P...</td>\n",
              "      <td>Mechanical</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>A Greenbelt-bound Green Line train at Branch ...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>A Greenbelt-bound Green Line train at Shaw-Ho...</td>\n",
              "      <td>Mechanical</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>A Largo Town Center-bound Blue Line train at ...</td>\n",
              "      <td>Mechanical</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65347</th>\n",
              "      <td>65347</td>\n",
              "      <td>65464</td>\n",
              "      <td>An inbound Blue Line train at Arlington Cemete...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65348</th>\n",
              "      <td>65348</td>\n",
              "      <td>65465</td>\n",
              "      <td>An inbound Orange Line train at New Carrollton...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65349</th>\n",
              "      <td>65349</td>\n",
              "      <td>65466</td>\n",
              "      <td>An outbound Orange Line train at Foggy Bottom ...</td>\n",
              "      <td>Environment</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65350</th>\n",
              "      <td>65350</td>\n",
              "      <td>65467</td>\n",
              "      <td>An outbound Red Line train at Tenleytown was t...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65351</th>\n",
              "      <td>65351</td>\n",
              "      <td>65468</td>\n",
              "      <td>An inbound Red Line train at Grosvenor did not...</td>\n",
              "      <td>Operational</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65352 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  Unnamed: 0.1  ...       Labels TrainOrTest\n",
              "0               0             0  ...  Operational        test\n",
              "1               1             1  ...   Mechanical       train\n",
              "2               2             2  ...  Operational       train\n",
              "3               3             3  ...   Mechanical        test\n",
              "4               4             4  ...   Mechanical       train\n",
              "...           ...           ...  ...          ...         ...\n",
              "65347       65347         65464  ...  Operational       train\n",
              "65348       65348         65465  ...  Operational       train\n",
              "65349       65349         65466  ...  Environment       train\n",
              "65350       65350         65467  ...  Operational       train\n",
              "65351       65351         65468  ...  Operational       train\n",
              "\n",
              "[65352 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glS9zx0e5535"
      },
      "source": [
        "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPWtwq0Y6RCK"
      },
      "source": [
        "#Test = df[df['TrainOrTest'] == 'test'].reset_index(drop = True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHag9aGr6bg0"
      },
      "source": [
        "#Train = df[df['TrainOrTest'] == 'train'].reset_index(drop = True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcJIHjIi6j_n"
      },
      "source": [
        "del df['TrainOrTest']"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yk0cBCwUQiy"
      },
      "source": [
        "df = df.rename(columns={'incident': 'tweet', 'Labels': 'label'})"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dqUXMQz7is0",
        "outputId": "7c9f3e36-4819-4c8b-df88-3d7038891878"
      },
      "source": [
        "!pip install contractions"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.48)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.2.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LCdTuScg6212",
        "outputId": "aa913fcc-e524-4a0e-ddcb-99d864e96a4d"
      },
      "source": [
        "#importing required libraries\n",
        "import nltk\n",
        "import inflect\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import re, string, unicodedata\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# First function is used to denoise text\n",
        "def denoise_text(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    text = soup.get_text()\n",
        "    text = contractions.fix(text)\n",
        "    return text\n",
        "sample_text = \"<p>he didn't say anything </br> about what's gonna <html> happen in the climax\"\n",
        "denoise_text(sample_text)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'he did not say anything  about what is going to  happen in the climax'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFesDthT7Idc",
        "outputId": "c289233d-3ef5-4c2c-d082-4fbb845d1b1c"
      },
      "source": [
        "# Text normalization includes many steps.\n",
        "# Each function below serves a step.\n",
        "nltk.download('stopwords')\n",
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "\n",
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words\n",
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words\n",
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words\n",
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems\n",
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n",
        "def normalize_text(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    words = remove_stopwords(words)\n",
        "    #words = stem_words(words)\n",
        "    words = lemmatize_verbs(words)\n",
        "    return words\n",
        "# Testing the functions\n",
        "print(\"remove_non_ascii results: \", remove_non_ascii(['h', 'ॐ', '©', '1']))\n",
        "print(\"to_lowercase results: \", to_lowercase(['HELLO', 'hiDDen', 'wanT', 'GOING']))\n",
        "print(\"remove_punctuation results: \", remove_punctuation(['hello!!', 'how?', 'done,']))\n",
        "print(\"replace_numbers results: \", replace_numbers(['1', '2', '3']))\n",
        "print(\"remove_stopwords results: \", remove_stopwords(['this', 'and', 'amazing']))\n",
        "print(\"stem_words results: \", stem_words(['beautiful', 'flying', 'waited']))\n",
        "print(\"lemmatize_verbs results: \", lemmatize_verbs(['hidden', 'walking', 'ran']))\n",
        "print(\"normalize_text results: \", normalize_text(['hidden', 'in', 'the', 'CAVES', 'he', 'WAited', '2', 'ॐ', 'hours!!']))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "remove_non_ascii results:  ['h', '', '', '1']\n",
            "to_lowercase results:  ['hello', 'hidden', 'want', 'going']\n",
            "remove_punctuation results:  ['hello', 'how', 'done']\n",
            "replace_numbers results:  ['one', 'two', 'three']\n",
            "remove_stopwords results:  ['amazing']\n",
            "stem_words results:  ['beauty', 'fly', 'wait']\n",
            "lemmatize_verbs results:  ['hide', 'walk', 'run']\n",
            "normalize_text results:  ['hide', 'cave', 'wait', 'two', 'hours']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aiQS-m78A7e",
        "outputId": "83b7bd32-9ffe-4650-cb27-87201298d962"
      },
      "source": [
        "# Tokenize tweet into words\n",
        "nltk.download('punkt')\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "# check the function\n",
        "sample_text = 'he did not say anything  about what is going to  happen'\n",
        "print(\"tokenize results :\", tokenize(sample_text))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "tokenize results : ['he', 'did', 'not', 'say', 'anything', 'about', 'what', 'is', 'going', 'to', 'happen']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "jjf-YzvF84Ta",
        "outputId": "32d574f1-19cb-4f3d-d797-45409dffee73"
      },
      "source": [
        "def text_prepare(text):\n",
        "    text = denoise_text(text)\n",
        "    text = ' '.join([x for x in normalize_text(tokenize(text))])\n",
        "    return text\n",
        "df['tweet'] = [text_prepare(x) for x in df['tweet']]\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fort tottenbound yellow line train braddock ro...</td>\n",
              "      <td>Operational</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>grosvenorbound red line train gallery place de...</td>\n",
              "      <td>Mechanical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>greenbeltbound green line train branch avenue ...</td>\n",
              "      <td>Operational</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>greenbeltbound green line train shawhoward off...</td>\n",
              "      <td>Mechanical</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>largo town centerbound blue line train stadium...</td>\n",
              "      <td>Mechanical</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               tweet        label\n",
              "0  fort tottenbound yellow line train braddock ro...  Operational\n",
              "1  grosvenorbound red line train gallery place de...   Mechanical\n",
              "2  greenbeltbound green line train branch avenue ...  Operational\n",
              "3  greenbeltbound green line train shawhoward off...   Mechanical\n",
              "4  largo town centerbound blue line train stadium...   Mechanical"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG7XS1a6c6xx"
      },
      "source": [
        "le = LabelEncoder()\n",
        "df['label'] = le.fit_transform(df['label'])"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcgbBv_7Vnqy",
        "outputId": "e851c9d9-ae14-45c4-bbb6-6b99670404cc"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-03 03:47:27--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-05-03 03:47:27--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-05-03 03:47:27--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.06MB/s    in 2m 40s  \n",
            "\n",
            "2021-05-03 03:50:07 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0WhiGmtVu_C",
        "outputId": "57cd4abf-ff7a-4dc7-9871-beef3073fb3d"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmORGu7uFwb_"
      },
      "source": [
        "from keras.layers import Dropout, Dense, Embedding, LSTM, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZScwJRoiQ4Vm",
        "outputId": "40379525-95b0-43cd-acf5-dca466f43dd9"
      },
      "source": [
        "def prepare_model_input(X_train, X_test,MAX_NB_WORDS=75000,MAX_SEQUENCE_LENGTH=500):\n",
        "    np.random.seed(7)\n",
        "    text = np.concatenate((X_train, X_test), axis=0)\n",
        "    text = np.array(text)\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "    tokenizer.fit_on_texts(text)\n",
        "    # pickle.dump(tokenizer, open('text_tokenizer.pkl', 'wb'))\n",
        "    # Uncomment above line to save the tokenizer as .pkl file \n",
        "    sequences = tokenizer.texts_to_sequences(text)\n",
        "    word_index = tokenizer.word_index\n",
        "    text = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "    indices = np.arange(text.shape[0])\n",
        "    # np.random.shuffle(indices)\n",
        "    text = text[indices]\n",
        "    print(text.shape)\n",
        "    X_train_Glove = text[0:len(X_train), ]\n",
        "    X_test_Glove = text[len(X_train):, ]\n",
        "    embeddings_dict = {}\n",
        "    f = open(\"glove.6B.50d.txt\", encoding=\"utf8\")\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "        except:\n",
        "            pass\n",
        "        embeddings_dict[word] = coefs\n",
        "    f.close()\n",
        "    print('Total %s word vectors.' % len(embeddings_dict))\n",
        "    return (X_train_Glove, X_test_Glove, word_index, embeddings_dict)\n",
        "## Check function\n",
        "x_train_sample = [\"Lorem Ipsum is simply dummy text of the printing and typesetting industry\", \"It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout\"]\n",
        "x_test_sample = [\"I’m creating a macro and need some text for testing purposes\", \"I’m designing a document and don’t want to get bogged down in what the text actually says\"]\n",
        "X_train_Glove_s, X_test_Glove_s, word_index_s, embeddings_dict_s = prepare_model_input(x_train_sample, x_test_sample, 100, 20)\n",
        "print(\"\\n X_train_Glove_s \\n \", X_train_Glove_s)\n",
        "print(\"\\n X_test_Glove_s \\n \", X_test_Glove_s)\n",
        "print(\"\\n Word index of the word testing is : \", word_index_s[\"testing\"])\n",
        "print(\"\\n Embedding for thw word want \\n \\n\", embeddings_dict_s[\"want\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 51 unique tokens.\n",
            "(4, 20)\n",
            "Total 400000 word vectors.\n",
            "\n",
            " X_train_Glove_s \n",
            "  [[ 0  0  0  0  0  0  0  0  8  9  5 10 11  2  6  3 12  4 13 14]\n",
            " [17 18 19  1 20 21 22 23 24  3 25 26  6  1 27 28 29 30 31 32]]\n",
            "\n",
            " X_test_Glove_s \n",
            "  [[ 0  0  0  0  0  0  0  0  0  7 33  1 34  4 35 36  2 37 38 39]\n",
            " [ 0  0  0  7 40  1 41  4 42 43 44 45 46 47 48 49  3  2 50 51]]\n",
            "\n",
            " Word index of the word testing is :  38\n",
            "\n",
            " Embedding for thw word want \n",
            " \n",
            " [ 0.13627  -0.054478  0.3703   -0.41574   0.60568  -0.42729  -0.50151\n",
            "  0.35923  -0.49154   0.21827  -0.15193   0.52536  -0.24206   0.023875\n",
            "  0.8225    1.089     0.98825  -0.17803   0.77806  -1.0647   -0.28742\n",
            "  0.50458   0.21612   0.65681   0.34295  -2.1084   -0.82557  -0.31966\n",
            "  0.87567  -1.0679    3.3802    1.2084   -1.272    -0.15921  -0.25237\n",
            " -0.2696   -0.18756  -0.35523   0.084172 -0.56539  -0.24081   0.15926\n",
            "  0.3287    0.54591   0.29897   0.18948  -0.57113   0.17399  -0.19338\n",
            "  0.51921 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0Z_SfPEWTJo"
      },
      "source": [
        "def build_bilstm(word_index, embeddings_dict, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5, hidden_layer = 3, lstm_node = 32):\n",
        "    model = Sequential()\n",
        "    # Make the embedding matrix using the embedding_dict\n",
        "    embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_dict.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found in embedding index will be all-zeros.\n",
        "            if len(embedding_matrix[i]) != len(embedding_vector):\n",
        "                print(\"could not broadcast input array from shape\", str(len(embedding_matrix[i])),\n",
        "                      \"into shape\", str(len(embedding_vector)), \" Please make sure your\"\n",
        "                                                                \" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\")\n",
        "                exit(1)\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    # Add embedding layer\n",
        "    model.add(Embedding(len(word_index) + 1,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights=[embedding_matrix],\n",
        "                                input_length=MAX_SEQUENCE_LENGTH,\n",
        "                                trainable=True))\n",
        "    # Add hidden layers \n",
        "    for i in range(0,hidden_layer):\n",
        "        # Add a bidirectional lstm layer\n",
        "        model.add(Bidirectional(LSTM(lstm_node, return_sequences=True, recurrent_dropout=0.2)))\n",
        "        # Add a dropout layer after each lstm layer\n",
        "        model.add(Dropout(dropout))\n",
        "    model.add(Bidirectional(LSTM(lstm_node, recurrent_dropout=0.2)))\n",
        "    model.add(Dropout(dropout))\n",
        "    # Add the fully connected layer with 256 nurons and relu activation\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    # Add the output layer with softmax activation since we have 2 classes\n",
        "    model.add(Dense(nclasses, activation='softmax'))\n",
        "    # Compile the model using sparse_categorical_crossentropy\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                      optimizer='adam',\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8Neq50AaY85",
        "outputId": "ddcc828f-ddef-4a17-ad2c-48599af9b2dc"
      },
      "source": [
        "X = df.tweet\n",
        "y = df.label\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
        "print(\"Preparing model input ...\")\n",
        "X_train_Glove, X_test_Glove, word_index, embeddings_dict = prepare_model_input(X_train,X_test)\n",
        "print(\"Done!\")\n",
        "print(\"Building Model!\")\n",
        "model = build_bilstm(word_index, embeddings_dict, 5)\n",
        "model.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing model input ...\n",
            "Found 824 unique tokens.\n",
            "(65352, 500)\n",
            "Total 400000 word vectors.\n",
            "Done!\n",
            "Building Model!\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 50)           41250     \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 500, 64)           21248     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_5 (Bidirection (None, 500, 64)           24832     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 500, 64)           24832     \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 500, 64)           0         \n",
            "_________________________________________________________________\n",
            "bidirectional_7 (Bidirection (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               16640     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 5)                 1285      \n",
            "=================================================================\n",
            "Total params: 154,919\n",
            "Trainable params: 154,919\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6BS1ue5a0vn"
      },
      "source": [
        "def get_eval_report(labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    precision = (tp)/(tp+fp)\n",
        "    recall = (tp)/(tp+fn)\n",
        "    f1 = (2*(precision*recall))/(precision+recall)\n",
        "    return {\n",
        "        \"mcc\": mcc,\n",
        "        \"true positive\": tp,\n",
        "        \"true negative\": tn,\n",
        "        \"false positive\": fp,\n",
        "        \"false negative\": fn,\n",
        "        \"pricision\" : precision,\n",
        "        \"recall\" : recall,\n",
        "        \"F1\" : f1,\n",
        "        \"accuracy\": (tp+tn)/(tp+tn+fp+fn)\n",
        "    }\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(labels, preds)\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efP0msqhaZdR",
        "outputId": "8dbbece2-ea2e-47a3-e885-eb27403e3539"
      },
      "source": [
        "history = model.fit(X_train_Glove, y_train,\n",
        "                           validation_data=(X_test_Glove,y_test),\n",
        "                           epochs=5,\n",
        "                           batch_size=128,\n",
        "                           verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "409/409 [==============================] - 2029s 5s/step - loss: 0.6565 - accuracy: 0.7581 - val_loss: 0.0626 - val_accuracy: 0.9858\n",
            "Epoch 2/5\n",
            "409/409 [==============================] - 2019s 5s/step - loss: 0.0583 - accuracy: 0.9856 - val_loss: 0.0287 - val_accuracy: 0.9945\n",
            "Epoch 3/5\n",
            "409/409 [==============================] - 2005s 5s/step - loss: 0.0265 - accuracy: 0.9943 - val_loss: 0.0173 - val_accuracy: 0.9963\n",
            "Epoch 4/5\n",
            " 66/409 [===>..........................] - ETA: 26:55 - loss: 0.0153 - accuracy: 0.9966"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WURcbeP6q2Ic"
      },
      "source": [
        "#https://medium.com/analytics-vidhya/building-a-text-classification-model-using-bilstm-c0548ace26f2\n",
        "#https://github.com/pashupati98/Text-Classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlcJW3Vva5eU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}